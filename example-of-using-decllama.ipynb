{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8645394,"sourceType":"datasetVersion","datasetId":5177894},{"sourceId":8773409,"sourceType":"datasetVersion","datasetId":5093807},{"sourceId":8775759,"sourceType":"datasetVersion","datasetId":5274437}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# This notebook was created to demonstrate the operation of an adapter for CodeLlama, designed to solve the problem of improving the readability of decompiled C code","metadata":{}},{"cell_type":"markdown","source":"# 1) Installing the necessary libraries","metadata":{}},{"cell_type":"code","source":"!pip install accelerate\n!pip install bitsandbytes\n!pip install peft\n!pip install transformers","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-06-24T19:54:44.475414Z","iopub.execute_input":"2024-06-24T19:54:44.475788Z","iopub.status.idle":"2024-06-24T19:55:44.060777Z","shell.execute_reply.started":"2024-06-24T19:54:44.475748Z","shell.execute_reply":"2024-06-24T19:55:44.059645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2) Import block","metadata":{}},{"cell_type":"code","source":"from datetime import datetime\nimport os\nimport sys\nimport torch\nfrom peft import get_peft_model, PeftModel\nfrom transformers import LlamaTokenizer, LlamaForCausalLM, BitsAndBytesConfig","metadata":{"execution":{"iopub.status.busy":"2024-06-24T19:55:50.322545Z","iopub.execute_input":"2024-06-24T19:55:50.323616Z","iopub.status.idle":"2024-06-24T19:55:57.369862Z","shell.execute_reply.started":"2024-06-24T19:55:50.323580Z","shell.execute_reply":"2024-06-24T19:55:57.368770Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3) Set device, basic model's tokenizer","metadata":{}},{"cell_type":"code","source":"# Set which device we will use (GPU or CPU).\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# The basic model\nBASE_MODEL = \"codellama/CodeLlama-7b-hf\"\n# Initializing the tokenizer\ntokenizer = LlamaTokenizer.from_pretrained(BASE_MODEL)\ntokenizer.pad_token_id = 0\ntokenizer.padding_side = \"left\"","metadata":{"execution":{"iopub.status.busy":"2024-06-24T19:56:01.811774Z","iopub.execute_input":"2024-06-24T19:56:01.812728Z","iopub.status.idle":"2024-06-24T19:56:03.077413Z","shell.execute_reply.started":"2024-06-24T19:56:01.812696Z","shell.execute_reply":"2024-06-24T19:56:03.076395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4) Upload our eval dataset with code examples (optional)","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\neval_dataset = load_dataset('csv', data_files='/kaggle/input/95131hr/train_dataset_HR.csv', split='train[0%:1%]')\nprint(len(eval_dataset))","metadata":{"execution":{"iopub.status.busy":"2024-06-24T19:56:08.628455Z","iopub.execute_input":"2024-06-24T19:56:08.628949Z","iopub.status.idle":"2024-06-24T19:56:20.421456Z","shell.execute_reply.started":"2024-06-24T19:56:08.628908Z","shell.execute_reply":"2024-06-24T19:56:20.420456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example of decompiler output:\nprint(eval_dataset[1]['HR'])\ntest = eval_dataset[1]['HR']","metadata":{"execution":{"iopub.status.busy":"2024-06-24T19:56:23.707214Z","iopub.execute_input":"2024-06-24T19:56:23.708194Z","iopub.status.idle":"2024-06-24T19:56:23.714744Z","shell.execute_reply.started":"2024-06-24T19:56:23.708159Z","shell.execute_reply":"2024-06-24T19:56:23.713712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5) Download the model together with DecLlama checkpoint","metadata":{}},{"cell_type":"code","source":"#load the basic model\nmodel = LlamaForCausalLM.from_pretrained(\n    BASE_MODEL,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    quantization_config=BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_compute_dtype=torch.float16,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type='nf4',\n    )\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T19:56:35.142741Z","iopub.execute_input":"2024-06-24T19:56:35.143117Z","iopub.status.idle":"2024-06-24T19:57:48.811863Z","shell.execute_reply.started":"2024-06-24T19:56:35.143090Z","shell.execute_reply":"2024-06-24T19:57:48.810747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load checkpoint\nmodel = PeftModel.from_pretrained(model, '/kaggle/input/norm63000')\nmodel.config.pad_token_id = tokenizer.pad_token_id = 0 \nmodel.config.bos_token_id = 1\nmodel.config.eos_token_id = 2","metadata":{"execution":{"iopub.status.busy":"2024-06-24T19:57:53.366388Z","iopub.execute_input":"2024-06-24T19:57:53.366873Z","iopub.status.idle":"2024-06-24T19:57:53.709800Z","shell.execute_reply.started":"2024-06-24T19:57:53.366838Z","shell.execute_reply":"2024-06-24T19:57:53.708441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Architecture of the model with an adapter\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T19:57:57.652960Z","iopub.execute_input":"2024-06-24T19:57:57.653326Z","iopub.status.idle":"2024-06-24T19:57:57.670709Z","shell.execute_reply.started":"2024-06-24T19:57:57.653299Z","shell.execute_reply":"2024-06-24T19:57:57.669682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6) Evaluation","metadata":{}},{"cell_type":"code","source":"#function for generating model prediction\ndef get_pred(dec, model, max_tokens):\n    eval_prompt = f\"\"\"You are a powerful decompiler model. Your job is to convert the ะก code decompiled using RetDec decompiler into a more human-readable form. That is, you should change the names of variables and functions and delete unnecessary parts of the code so that it looks more like the source code of a C program. You are given a ะก code decompiled using RetDec decompiler. You must output the source code of a C program.\n\n### Your input:\n{dec}\n### The original C program:\n        \"\"\"\n    model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n    model.eval()\n    with torch.no_grad():\n        pred = (tokenizer.decode(model.generate(**model_input, max_new_tokens=max_tokens)[0], skip_special_tokens=True))\n    j = 0\n    while (pred[j]+pred[j+1]+pred[j+2]+pred[j+3]+pred[j+4])!='### T':\n        j+=1\n    pred=pred[j::]\n    pred = pred[39::]\n    return (pred)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T19:58:21.549977Z","iopub.execute_input":"2024-06-24T19:58:21.550395Z","iopub.status.idle":"2024-06-24T19:58:21.559462Z","shell.execute_reply.started":"2024-06-24T19:58:21.550357Z","shell.execute_reply":"2024-06-24T19:58:21.558385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = get_pred(test,model,1000)\nprint(pred)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T19:58:26.542641Z","iopub.execute_input":"2024-06-24T19:58:26.543055Z","iopub.status.idle":"2024-06-24T20:00:41.120431Z","shell.execute_reply.started":"2024-06-24T19:58:26.543028Z","shell.execute_reply":"2024-06-24T20:00:41.119407Z"},"trusted":true},"execution_count":null,"outputs":[]}]}